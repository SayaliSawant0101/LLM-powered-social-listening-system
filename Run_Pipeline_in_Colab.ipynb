{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM-powered Social Listening System - Pipeline Runner\n",
        "\n",
        "This notebook runs the complete data pipeline in Google Colab with GPU support.\n",
        "\n",
        "## Steps:\n",
        "1. Install dependencies\n",
        "2. Set up AWS credentials\n",
        "3. Fetch data from Athena\n",
        "4. Run sentiment analysis (Stage 1)\n",
        "5. Run aspect analysis (Stage 2)\n",
        "6. Generate themes parquet (Stage 3)\n",
        "7. Download results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional - to save files)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q boto3 pandas pyathena pyarrow python-dotenv transformers sentence-transformers scikit-learn torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Enable GPU: Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Up AWS Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up AWS credentials\n",
        "import os\n",
        "\n",
        "# ‚ö†Ô∏è SECURITY: Use Colab Secrets or environment variables - DO NOT hardcode credentials!\n",
        "# Option 1: Use Colab Secrets (Recommended)\n",
        "# from google.colab import userdata\n",
        "# os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "# os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "# Option 2: Set manually (ONLY for local testing, remove before committing)\n",
        "# os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY_HERE'\n",
        "# os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY_HERE'\n",
        "\n",
        "# Required AWS configuration\n",
        "os.environ['AWS_REGION'] = 'us-east-1'\n",
        "os.environ['AWS_SESSION_TOKEN'] = ''  # Leave empty if not using temporary credentials\n",
        "\n",
        "# Athena configuration\n",
        "os.environ['ATHENA_SCHEMA'] = 'cs668_capstone'\n",
        "os.environ['ATHENA_WORKGROUP'] = 'primary'\n",
        "os.environ['ATHENA_STAGING_DIR'] = 's3://capstone-transformed-twitterdata-cs668/query_results/'\n",
        "# Increase result size from 10k to 15k\n",
        "os.environ['ATHENA_SQL'] = 'SELECT DISTINCT * FROM tweet_clean LIMIT 15000'\n",
        "\n",
        "# Verify credentials are set\n",
        "if 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ:\n",
        "    print(\"‚úÖ AWS credentials configured\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Warning: AWS credentials not set. Please configure them above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Project Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "print(\"‚úÖ Directory structure created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fetch Data from AWS Athena\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyathena import connect\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create connection\n",
        "conn = connect(\n",
        "    s3_staging_dir=os.getenv('ATHENA_STAGING_DIR'),\n",
        "    region_name=os.getenv('AWS_REGION'),\n",
        "    work_group=os.getenv('ATHENA_WORKGROUP'),\n",
        "    schema_name=os.getenv('ATHENA_SCHEMA'),\n",
        "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
        "    aws_session_token=os.getenv('AWS_SESSION_TOKEN') or None,\n",
        ")\n",
        "\n",
        "# Execute query\n",
        "print(f\"üìä Executing query: {os.getenv('ATHENA_SQL')}\")\n",
        "df = pd.read_sql(os.getenv('ATHENA_SQL'), conn)\n",
        "print(f\"‚úÖ Retrieved {len(df):,} rows, {len(df.columns)} columns\")\n",
        "print(f\"\\nColumns: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Stage 0 parquet\n",
        "df.to_parquet('data/tweets_stage0_raw.parquet', index=False)\n",
        "print(f\"‚úÖ Saved Stage 0: data/tweets_stage0_raw.parquet ({len(df):,} rows)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Stage 1 - Sentiment Analysis (GPU Accelerated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Stage 0 data\n",
        "import pandas as pd\n",
        "df = pd.read_parquet('data/tweets_stage0_raw.parquet')\n",
        "print(f\"üìä Loaded {len(df):,} rows\")\n",
        "\n",
        "# Run sentiment analysis\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "text_col = 'clean_tweet' if 'clean_tweet' in df.columns else ('text' if 'text' in df.columns else df.columns[2])\n",
        "print(f\"Using text column: {text_col}\")\n",
        "\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    device=device,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "texts = df[text_col].astype(str).tolist()\n",
        "batch_size = 32 if device == 0 else 8\n",
        "\n",
        "labels = []\n",
        "scores = []\n",
        "t0 = time.time()\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    if i % (batch_size * 50) == 0:\n",
        "        print(f\"Progress: {i}/{len(texts)} ({100*i/len(texts):.1f}%) | Elapsed: {time.time()-t0:.1f}s\")\n",
        "    batch = texts[i:i+batch_size]\n",
        "    results = sentiment_pipeline(batch, truncation=True, max_length=256)\n",
        "    for row in results:\n",
        "        best = max(row, key=lambda x: x['score'])\n",
        "        labels.append(best['label'].lower())\n",
        "        scores.append(best['score'])\n",
        "\n",
        "df['sentiment_label'] = labels\n",
        "df['sentiment_score'] = scores\n",
        "\n",
        "print(f\"\\n‚úÖ Sentiment analysis complete!\")\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "# Save Stage 1\n",
        "df.to_parquet('data/tweets_stage1_sentiment.parquet', index=False)\n",
        "print(f\"\\n‚úÖ Saved Stage 1: data/tweets_stage1_sentiment.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Stage 1 data\n",
        "df = pd.read_parquet('data/tweets_stage1_sentiment.parquet')\n",
        "print(f\"üìä Loaded {len(df):,} rows\")\n",
        "\n",
        "# Aspect analysis\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "aspects = [\"pricing\", \"delivery\", \"returns\", \"staff\", \"app/ux\"]\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "aspect_pipeline = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "text_col = 'clean_tweet' if 'clean_tweet' in df.columns else ('text' if 'text' in df.columns else df.columns[2])\n",
        "texts = df[text_col].astype(str).tolist()\n",
        "batch_size = 24 if device == 0 else 8\n",
        "\n",
        "scores_per_aspect = {a: [] for a in aspects}\n",
        "t0 = time.time()\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    if i % (batch_size * 20) == 0:\n",
        "        print(f\"Progress: {i}/{len(texts)} ({100*i/len(texts):.1f}%) | Elapsed: {time.time()-t0:.1f}s\")\n",
        "    batch = texts[i:i+batch_size]\n",
        "    preds = aspect_pipeline(batch, candidate_labels=aspects, multi_label=True, truncation=True)\n",
        "    if isinstance(preds, dict):\n",
        "        preds = [preds]\n",
        "    for p in preds:\n",
        "        l2s = dict(zip(p['labels'], p['scores']))\n",
        "        for a in aspects:\n",
        "            scores_per_aspect[a].append(float(l2s.get(a, 0.0)))\n",
        "\n",
        "for a in aspects:\n",
        "    df[f'aspect_{a.replace(\"/\", \"_\")}'] = scores_per_aspect[a]\n",
        "\n",
        "arr = df[[f'aspect_{a.replace(\"/\", \"_\")}' for a in aspects]].values\n",
        "idxmax = arr.argmax(axis=1)\n",
        "maxval = arr.max(axis=1)\n",
        "dom = np.where(maxval >= 0.5, np.array([a.replace(\"/\", \"_\") for a in aspects])[idxmax], 'none')\n",
        "df['aspect_dominant'] = dom\n",
        "\n",
        "print(f\"\\n‚úÖ Aspect analysis complete!\")\n",
        "print(f\"\\nAspect distribution:\")\n",
        "print(df['aspect_dominant'].value_counts())\n",
        "\n",
        "# Save Stage 2\n",
        "df.to_parquet('data/tweets_stage2_aspects.parquet', index=False)\n",
        "print(f\"\\n‚úÖ Saved Stage 2: data/tweets_stage2_aspects.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Stage 2 data\n",
        "df = pd.read_parquet('data/tweets_stage2_aspects.parquet')\n",
        "print(f\"üìä Loaded {len(df):,} rows\")\n",
        "\n",
        "# Theme generation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "text_col = 'clean_tweet' if 'clean_tweet' in df.columns else ('text' if 'text' in df.columns else df.columns[2])\n",
        "texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "# Create embeddings (TF-IDF)\n",
        "print(\"Creating embeddings...\")\n",
        "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1, 2))\n",
        "embeddings = vectorizer.fit_transform(texts).astype('float32')\n",
        "embeddings = normalize(embeddings)\n",
        "\n",
        "# Clustering\n",
        "print(\"Clustering...\")\n",
        "n_clusters = 6\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=3)\n",
        "df['theme'] = kmeans.fit_predict(embeddings)\n",
        "\n",
        "print(f\"\\n‚úÖ Theme clustering complete!\")\n",
        "print(f\"\\nTheme distribution:\")\n",
        "print(df['theme'].value_counts().sort_index())\n",
        "\n",
        "# Save Stage 3 (THIS IS WHAT YOUR SERVER NEEDS!)\n",
        "df.to_parquet('data/tweets_stage3_themes.parquet', index=False)\n",
        "print(f\"\\n‚úÖ Saved Stage 3: data/tweets_stage3_themes.parquet\")\n",
        "print(f\"\\nüéâ This is the file your server needs!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Download the important parquet files\n",
        "files_to_download = [\n",
        "    'data/tweets_stage3_themes.parquet',  # Most important - for server\n",
        "    'data/tweets_stage2_aspects.parquet',\n",
        "    'data/tweets_stage1_sentiment.parquet',\n",
        "    'data/tweets_stage0_raw.parquet',\n",
        "]\n",
        "\n",
        "for file in files_to_download:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"üì• Downloading {file}...\")\n",
        "        files.download(file)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  {file} not found\")\n",
        "\n",
        "print(\"\\n‚úÖ All files downloaded!\")\n",
        "print(\"\\nüìã Next steps:\")\n",
        "print(\"   1. Upload tweets_stage3_themes.parquet to your server's data/ directory\")\n",
        "print(\"   2. Restart your server\")\n",
        "print(\"   3. Test the /api/themes/:id/tweets endpoint\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
